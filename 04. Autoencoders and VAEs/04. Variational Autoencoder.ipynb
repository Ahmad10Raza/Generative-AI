{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Variational Autoencoder (VAE) is a type of autoencoder that introduces a probabilistic approach to autoencoding. It's a generative model that provides a probabilistic manner for describing an observation in latent space. \n",
    "\n",
    "Unlike a traditional autoencoder, which maps the input onto a latent vector, a VAE maps the input data into the parameters of a probability distribution, such as the mean and variance of a Gaussian. This approach produces a continuous, structured latent space, which is useful for using and manipulating the data.\n",
    "\n",
    "The architecture of a VAE is similar to a traditional autoencoder consisting of an encoder, a decoder, and a loss function. However, the encoder and decoder are connected through a bottleneck layer that imposes a probabilistic constraint, ensuring that the representations in the latent space follow a Gaussian distribution.\n",
    "\n",
    "The loss function of a VAE consists of two parts: a reconstruction loss that forces the decoded samples to match the initial inputs, and a KL divergence that measures how much the latent variables deviate from the standard normal distribution.\n",
    "\n",
    "VAEs are widely used for generating new data that resemble the training data, such as generating new images that look like the images in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arcitecture Of  Variational Autoencoder?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
